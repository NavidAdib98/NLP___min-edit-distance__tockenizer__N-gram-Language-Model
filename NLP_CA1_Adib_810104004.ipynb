{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">⚜️━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⚜️<br>تمرین 1<br>⚜️━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⚜️</h1>\n",
    "<h2 dir='rtl' style=\"color:rgb(90, 255, 184); font-size: 20px;\">آشنایی با توکنایزرها و N-gram</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">شهرزاد آذری آزاد - فرشاد حسامی</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">shahrzad.azari@ut.ac.ir - farshad.hessami@ut.ac.ir</p>\n",
    "\n",
    "<div dir='rtl' style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p dir='rtl' style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">📝 مشخصات دانشجو:</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">نام و نام خانوادگی: نوید ادیب</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">شماره دانشجویی: 810104004</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">تاریخ ارسال: {{تاریخ_ارسال}}</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir; max-width: 100%;word-wrap: break-word;\">\n",
    "<div style=\"line-height: 2.0; font-size: 17px; color: black; font-family: Vazir;\">\n",
    "<div style=\"padding-right:40px\">\n",
    "در بخش‌های مختلف این تمرین با مفاهیم Tokenization, Regular Expression , N-gram Language Modeling آشنا می‌شوید و آن‌ها را پیاده‌سازی می‌کنید. \n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding-right:100px\">\n",
    "📋 <b>ساختار تمرین:</b>\n",
    "<li><b>سوال اول - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)</b></li>\n",
    "<ul>\n",
    "<li>بخش اول: تشخیص ایمیل‌های قابل قبول با Regex</li>\n",
    "<li>بخش دوم: پیاده‌سازی Auto-Correction با Minimum Edit Distance</li>\n",
    "</ul>\n",
    "<li><b>سوال دوم - <span dir=\"ltr\">Tokenization</span> (25)</b></li>\n",
    "<ul>\n",
    "<li>بخش اول: Rule-based Tokenizer</li>\n",
    "<li>بخش دوم: BPE Tokenizer</li>\n",
    "<li>بخش سوم: Wordpiece Tokenizer</li>\n",
    "<li>بخش چهارم: Tokenization Visualization</li>\n",
    "</ul>\n",
    "<li><b>سوال سوم - <span dir=\"ltr\">N-gram Language Modeling</span> (55)</b></li>\n",
    "<ul>\n",
    "<li>بخش اول: Data cleaning & Tokenization</li>\n",
    "<li>بخش دوم: پیاده‌سازی N-gram</li>\n",
    "<li>بخش سوم: معیار Perplexity</li>\n",
    "<li>بخش چهارم: روش‌های هموارسازی</li>\n",
    "<li>بخش پنجم: شبیه‌سازی Temperature با روش‌های هموارسازی</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "💡 <b>نکات مهم:</b>\n",
    "<br>\n",
    "در متن سوالات، بخش‌هایی که در آن‌ها مجاز به استفاده از کتابخانه‌های آماده هستید ذکر شده است.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">⚜️━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⚜️<br>سوال اول - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)<br>⚜️━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⚜️</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "در این سوال شما با نمونه‌هایی عملی از استفاده‌ی Regex و همینطور Minimum Distance مواجه می‌شوید. در بخش اول این سوال، شما باید ایمیل‌های قابل‌قبول را تشخیص داده و در بخش دوم سوال شما با استفاده از Minimum Distance یک سیستم Auto-Correction ساده را پیاده‌سازی خواهید کرد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">تشخیص ایمیل‌های قابل قبول با Regex</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "فایل emails.txt که در اختیار شما قرار داده‌شده، شامل تعدادی اسم به‌همراه ایمیل ثبت‌شده‌‌شان در یک سامانه می‌باشد.\n",
    "<br>\n",
    "از شما خواسته‌شده است تا ایمیل‌هایی که معتبر هستند را با استفاده از regex مشخص کنید.\n",
    "<br>\n",
    "ایمیل از دو بخش تشکیل می‌شود. که با @ از هم جدا می‌شوند. بخش اول (قبل از @) local-part نام دارد و بخش دوم domain.\n",
    "<br>\n",
    "منظور از ایمیل معتبر این است که موارد زیر در آن‌ها رعایت شده‌باشند:\n",
    "<br>\n",
    "۱. دو بخش ایمیل با یک و تنها یک @ از هم جدا شده‌باشند.\n",
    "<br>\n",
    "۲. در local-part هم نام و هم نام خانوادگی شخص وجود داشته باشد.\n",
    "<br>\n",
    "۳. در local-part تنها حروف انگلیسی، اعداد و کاراکترهای -،_ و . مجاز هستند. همچنین دو نقطه نمی‌توانند پشت هم بیایند.\n",
    "<br>\n",
    "۴. در بخش domain یک میزبان داریم و یک پسوند. میزبان و پسوند همیشه با یک نقطه از یکدیگر جدا می‌شوند. (میزبان می‌تواند در خود نقطه داشته باشد، اما دو نقطه‌ی متوالی در domain مجاز نیست.)\n",
    "<br>\n",
    "۵. پسوند از حروف انگلیسی تشکیل می‌شود و حداقل دو کاراکتر دارد.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- لیست ایمیل‌های قابل قبول موجود در فایل emails.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Valid Emails:\n",
      "1: milad.fattahi@uni.ir\n",
      "2: parsa.golkar@host.ir\n",
      "3: moradi.reza@uni.ir\n",
      "4: niloufarheidari@lab.ir\n",
      "5: mahsa_akbari@domain.com\n",
      "6: ali.rezaei@example.com\n",
      "7: zarei-farhad@host.ir\n",
      "8: hamed.jalali@net.com\n",
      "9: ramin.shafiei@project.net\n",
      "10: sadeghi.laleh@work.ir\n",
      "11: nouri.pouya@mail.ir\n",
      "12: elhamdavari@office.net\n",
      "13: behnam.khatibi@mail.org\n",
      "14: sara.soleimani@host.ir\n",
      "15: shirin_hashemi@domain.com\n",
      "16: arman.khalili@school.ir\n",
      "17: rahaesfandiari@company.com\n",
      "18: kiana.ghasemi@uni.edu\n",
      "19: navid_khodadadi@research.org\n",
      "20: mehrdad.ebrahimi@example.org\n",
      "21: raminshafiei@project.net\n",
      "22: hamedjalali@domain.org\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Regex pattern نهایی\n",
    "# -----------------------------\n",
    "pattern = re.compile(\n",
    "    r'^'                                    # ابتدای رشته\n",
    "    r'(?!.*\\.\\.)'                            # جلوگیری از دو نقطه پشت سر هم در کل رشته\n",
    "    r'name=([A-Za-z]+)\\s+([A-Za-z]+),'      # capture نام و نام خانوادگی\n",
    "    r'\\s*email='\n",
    "    r'(?:(?=.*\\1)(?=.*\\2)[A-Za-z0-9._-]*)' # local-part شامل نام و نام خانوادگی، حروف، اعداد و کاراکترهای مجاز\n",
    "    r'@[A-Za-z0-9]+(?:\\.[A-Za-z0-9]+)*'    # دامنه و زیردامنه‌ها\n",
    "    r'\\.[A-Za-z]{2,}'                       # پسوند حداقل دو حرف\n",
    "    r'$',\n",
    "    re.IGNORECASE                           # حساسیت به بزرگ و کوچک بودن حروف حذف شده\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# لیست برای ایمیل‌های معتبر\n",
    "# -----------------------------\n",
    "valid_emails = []\n",
    "\n",
    "# -----------------------------\n",
    "# خواندن فایل emails.txt و بررسی هر خط\n",
    "# -----------------------------\n",
    "with open(\"emails.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()  # حذف فاصله و newline اضافی\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            # گرفتن ایمیل از خط\n",
    "            email = match.group(3) if len(match.groups()) > 2 else line.split(\"email=\")[1]\n",
    "            valid_emails.append(email)\n",
    "\n",
    "# -----------------------------\n",
    "# چاپ ایمیل‌های معتبر\n",
    "# -----------------------------\n",
    "print(\"✅ Valid Emails:\")\n",
    "a =1\n",
    "for email in valid_emails:\n",
    "    print(f\"{a}: {email}\")\n",
    "    a+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "در این بخش باید با استفاده از Regex یک تابع بنویسید که اسم یک شخص را به‌عنوان ورودی دریافت کند و درصورت وجود ایمیل معتبر برای این اسم در بین ایمیل‌های ثبت‌شده، ایمیل را برگرداند. در غیر این‌صورت، یک پیغام عدم وجود چاپ کند.\n",
    "<br>\n",
    "توجه: ممکن است ایمیل این اشخاص، تحت نام شخص دیگری ثبت شده باشد. کار شما این است که ایمیل معتبر این افراد را از بین تمام ایمیل‌ها پیدا کنید.\n",
    "<br>\n",
    "تابع را با ورودی‌های زیر اجرا کنید:\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name1 = Behnam Khatibi\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name2 = Mehrdad Ebrahimi\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- ایمیل‌های معتبر مربوط به افراد ذکر شده در توضیحات\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behnam.khatibi@mail.org\n",
      "mehrdad.ebrahimi@example.org\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import re\n",
    "\n",
    "# valid_emails assume we have this from previous part\n",
    "\n",
    "def find_email_by_name(full_name, email_list):\n",
    "    \"\"\"\n",
    "    ورودی: full_name (str) و email_list (list)\n",
    "    خروجی: ایمیل معتبر یا پیغام عدم وجود\n",
    "    \"\"\"\n",
    "    # جدا کردن نام و نام خانوادگی\n",
    "    parts = full_name.strip().split()\n",
    "    if len(parts) != 2:\n",
    "        return \"⚠ نام کامل باید شامل نام و نام خانوادگی باشد\"\n",
    "    \n",
    "    first_name, last_name = parts\n",
    "    # الگوی regex برای local-part\n",
    "    # بررسی وجود هر دو نام در local-part ایمیل\n",
    "    pattern = re.compile(\n",
    "        rf'(?=.*{re.escape(first_name)})(?=.*{re.escape(last_name)})',    # chatgpt told me to use scape -> for safty but we can ignore it\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # جستجو در لیست ایمیل‌های معتبر\n",
    "    for email in email_list:\n",
    "        local_part = email.split('@')[0]\n",
    "        if pattern.search(local_part):\n",
    "            return email  # ایمیل پیدا شد\n",
    "    \n",
    "    return f\"ایمیل معتبری برای '{full_name}' پیدا نشد.\"\n",
    "\n",
    "# -----------------------------\n",
    "# تست تابع با ورودی‌ها\n",
    "# -----------------------------\n",
    "name1 = \"Behnam Khatibi\"\n",
    "name2 = \"Mehrdad Ebrahimi\"\n",
    "\n",
    "print(find_email_by_name(name1, valid_emails))\n",
    "print(find_email_by_name(name2, valid_emails))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">پیاده‌سازی Auto-Correction با Minimum Edit Distance</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "۱. ابتدا الگوریتم levenshtein_distance را پیاده‌سازی کنید و با استفاده از آن minimum_distance بین کلمات زیر را به‌دست آورید.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair1 = \"Athletic\", \"Atlantic\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair2 = \"London\", \"Boston\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"Action\", \"Compact\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"\", \"Sting\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- مقدار minimum distance بین جفت کلمه‌های داده‌شده در بخش توضیحات\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1: 'Athletic' ↔ 'Atlantic' → Weighted Distance = 4\n",
      "Pair 2: 'London' ↔ 'Boston' → Weighted Distance = 6\n",
      "Pair 3: 'Action' ↔ 'Compact' → Weighted Distance = 9\n",
      "Pair 4: '' ↔ 'Sting' → Weighted Distance = 5\n"
     ]
    }
   ],
   "source": [
    "def minimum_edit_distance(s1, s2):\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # initialize first row and column\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # fill DP table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            left = dp[i - 1][j] + 1         # insertion\n",
    "            down = dp[i][j - 1] + 1         # deletion\n",
    "            diag = dp[i - 1][j - 1] + (0 if s1[i - 1] == s2[j - 1] else 2)  # substitution\n",
    "\n",
    "            dp[i][j] = min(left, down, diag)\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "# test cases\n",
    "pairs = [\n",
    "    (\"Athletic\", \"Atlantic\"),\n",
    "    (\"London\", \"Boston\"),\n",
    "    (\"Action\", \"Compact\"),\n",
    "    (\"\", \"Sting\")\n",
    "]\n",
    "\n",
    "for i, (s1, s2) in enumerate(pairs, 1):\n",
    "    dist = minimum_edit_distance(s1, s2)\n",
    "    print(f\"Pair {i}: '{s1}' ↔ '{s2}' → Weighted Distance = {dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "۲. بعد از پیاده‌سازی minimum distance حال باید با استفاده از آن، جمله‌ی زیر را اصلاح املایی کنید. در این جمله تعدادی کلمه وجود دارند که املایشان نادرست است. یک لیست از املای صحیح کلمات که کلمات این جمله را نیز شامل می‌شوند در فایل vocab.txt موجود هستند.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "sentence_to_be_corrected = \n",
    "\"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- اصلاح‌شده‌ی جمله‌ی داده‌شده در بخش توضیحات با کمک فایل vocab.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:\n",
      "['Python', 'first', 'algorithm', 'distance', 'edit', 'code', 'function', 'variable', 'loop', 'condition', 'really', 'string', 'integer', 'list', 'array', 'input', 'output', 'compile', 'execute', 'they', 'debug', 'research', 'project', 'experiment', 'analysis', 'data', 'are', 'result', 'model', 'computation', 'and', 'simulation', 'performance', 'accuracy', 'enjoy', 'precision', 'write', 'read', 'run', 'test', 'learn', 'create', 'university', 'study', 'improve', 'understand', 'compare', 'their', 'evaluate', 'correct', 'it', 'students', 'efficient', 'accurate', 'fast', 'slow', 'easy', 'hard', 'interesting', 'enjoyable', 'practical', 'hello', 'writing']\n",
      "string words:\n",
      "['helo', 'studnts', 'at', 'the', 'universty', 'are', 'wrting', 'ther', 'frst', 'edit', 'distnce', 'algorthm', 'in', 'pythn,', 'and', 'they', 'reely', 'enjy', 'it!']\n",
      "helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\n",
      "corrected sentence:\n",
      "hello students at they university are writing their first edit distance algorithm in Python, and they reely enjoy it!\n"
     ]
    }
   ],
   "source": [
    "input_string = \"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "\n",
    "with open(\"vocab.txt\", \"r\") as f:\n",
    "    vocab_line = f.readline()\n",
    "    vocab = [word.strip() for word in vocab_line.split(\",\") if word.strip()]\n",
    "print(\"vocab:\")\n",
    "print(vocab)\n",
    "\n",
    "words = input_string.split()\n",
    "print(\"string words:\")\n",
    "print(words)\n",
    "\n",
    "corrected = []\n",
    "for word in words:\n",
    "    clean_word = ''.join(ch for ch in word if ch.isalpha())\n",
    "    suffix = ''.join(ch for ch in word if not ch.isalpha())\n",
    "\n",
    "    if not clean_word:\n",
    "        corrected.append(word)\n",
    "        continue\n",
    "\n",
    "    min_dist = float(\"inf\")\n",
    "    best_match = clean_word\n",
    "\n",
    "    for v in vocab:\n",
    "        dist = minimum_edit_distance(clean_word.lower(), v.lower())\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_match = v\n",
    "    if min_dist < 2:\n",
    "        corrected.append(best_match + suffix)\n",
    "    else:\n",
    "        corrected.append(clean_word + suffix)\n",
    "\n",
    "print(input_string)\n",
    "print(\"corrected sentence:\")\n",
    "print(' '.join(corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">⚜️━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⚜️<br>سوال دوم - <span dir=\"ltr\">Tokenization</span> (25)<br>⚜️━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⚜️</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "در این سوال، با انواع مختلف توکنایزر و روش  پیاده‌سازی آن‌ها آشنا می‌شوید.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Rule-based Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "با کمک دستورات regex یک توکنایزر بنویسید که متن را با استفاده از علائم نگارشی تقسیم کند.\n",
    "<br>\n",
    "سپس عملکرد آن را بر روی جملات داده شده زیر آزمایش کنید.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- تعداد و لیست توکن‌های ایجادشده با توکنایزر صورت سوال برای هر جمله<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sentence 1: ['Hello', ',', 'world', '!', 'NLP', 'is', 'fun', '.']\n",
      "Tokens sentence 2: ['That', 'U', '.', 'S', '.', 'A', '.', 'poster', '-', 'print', 'costs', '$', '12', '.', '40', '.', '.', '.']\n",
      "\n",
      "Vocabulary: {'Hello': 0, ',': 1, 'world': 2, '!': 3, 'NLP': 4, 'is': 5, 'fun': 6, '.': 7, 'That': 8, 'U': 9, 'S': 10, 'A': 11, 'poster': 12, '-': 13, 'print': 14, 'costs': 15, '$': 16, '12': 17, '40': 18}\n",
      "Vocabulary length: 19\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import re\n",
    "class RuleBasedTokenizer:\n",
    "    def __init__(self):\n",
    "        # initial known tokens and punctuation\n",
    "        self.token_to_id = {}\n",
    "        self.next_id = 0\n",
    "        self.pattern = r\"[A-Za-z]+|[.,!?$-]|\\d+\"  # words, punctuation, numbers like 12.40\n",
    "\n",
    "    def _add_token(self, token):\n",
    "        \"\"\"Add token to vocabulary if not seen before\"\"\"\n",
    "        if token not in self.token_to_id:\n",
    "            self.token_to_id[token] = self.next_id\n",
    "            self.next_id += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize input text and update vocabulary\"\"\"\n",
    "        tokens = re.findall(self.pattern, text)\n",
    "        for tok in tokens:\n",
    "            self._add_token(tok)\n",
    "        return tokens\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Return the vocabulary\"\"\"\n",
    "        return self.token_to_id\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tokenizer = RuleBasedTokenizer()\n",
    "\n",
    "s1 = \"Hello, world! NLP is fun.\"\n",
    "s2 = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "print(\"Tokens sentence 1:\", tokenizer.tokenize(s1))\n",
    "print(\"Tokens sentence 2:\", tokenizer.tokenize(s2))\n",
    "print(\"\\nVocabulary:\", tokenizer.get_vocab())\n",
    "print(\"Vocabulary length:\",tokenizer.get_vocab().__len__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ایرادات این توکنایزر چیست؟\n",
    "<br>\n",
    "چند راه‌حل برای بهبود عملکرد این توکنایزر ارائه دهید.\n",
    "<br>\n",
    "یکی از آن‌ها را پیاده‌سازی کنید.\n",
    "<br>\n",
    "عملکرد توکنایزر جدید را بر روی جملات داده شده آزمایش کنید.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- تعداد و لیست توکن‌های ایجادشده با توکنایزر بهبودیافته برای هر جمله\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens sentence 1: ['Hello', ',', 'world', '!', 'NLP', 'is', 'fun', '.']\n",
      "Tokens sentence 2: ['That', 'U.S.A.', 'poster-print', 'costs', '$', '12.40', '...']\n",
      "\n",
      "Vocabulary: {'Hello': 0, ',': 1, 'world': 2, '!': 3, 'NLP': 4, 'is': 5, 'fun': 6, '.': 7, 'That': 8, 'U.S.A.': 9, 'poster-print': 10, 'costs': 11, '$': 12, '12.40': 13, '...': 14}\n",
      "Vocabulary length: 15\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class RuleBasedTokenizer:\n",
    "    def __init__(self, case_sensitive=True):\n",
    "        self.token_to_id = {}\n",
    "        self.next_id = 0\n",
    "        self.case_sensitive = case_sensitive\n",
    "        \n",
    "        # Improved regex pattern for smarter tokenization\n",
    "        self.pattern = (\n",
    "            r\"[A-Z](?:\\.[A-Z])+\\.?\"      # Abbreviations like U.S.A.\n",
    "            r\"|[0-9]+\\.[0-9]+\"           # Decimal numbers like 12.40\n",
    "            r\"|[A-Za-z]+(?:-[A-Za-z]+)*\" # Hyphenated words like poster-print\n",
    "            r\"|\\.{2,}\"                   # Ellipses (...)\n",
    "            r\"|[.,!?$]\"                  # Punctuation or symbols\n",
    "        )\n",
    "\n",
    "    def _add_token(self, token):\n",
    "        if token not in self.token_to_id:\n",
    "            self.token_to_id[token] = self.next_id\n",
    "            self.next_id += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if not self.case_sensitive:\n",
    "            text = text.lower()\n",
    "        tokens = re.findall(self.pattern, text)\n",
    "        for tok in tokens:\n",
    "            self._add_token(tok)\n",
    "        return tokens\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.token_to_id\n",
    "\n",
    "\n",
    "# Example usage\n",
    "tokenizer = RuleBasedTokenizer()\n",
    "\n",
    "s1 = \"Hello, world! NLP is fun.\"\n",
    "s2 = \"That U.S.A. poster-print costs $12.40...\"\n",
    "\n",
    "print(\"Tokens sentence 1:\", tokenizer.tokenize(s1))\n",
    "print(\"Tokens sentence 2:\", tokenizer.tokenize(s2))\n",
    "print(\"\\nVocabulary:\", tokenizer.get_vocab())\n",
    "print(\"Vocabulary length:\", len(tokenizer.get_vocab()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e;margin:5px; padding:20px; border-radius:8px; color:#111'>\n",
    "✍️ <b>پاسخ تشریحی زیربخش اول:</b><br>\n",
    "\n",
    "ما یک <b>توکنایزر مبتنی بر قوانین (Rule-Based)</b> طراحی کردیم تا متن را به توکن‌های کلمات و علائم نگارشی تقسیم کند.\n",
    "\n",
    "<strong>نمونه ورودی:</strong><br>\n",
    "\"Hello, world! NLP is fun.\"<br>\n",
    "\"That U.S.A. poster-print costs $12.40...\"<br><br>\n",
    "\n",
    "<strong>روش اولیه:</strong><br>\n",
    "الگوی ساده استفاده شده قادر به تشخیص کلمات و علائم نگارشی بود، اما در موارد زیر مشکل داشت:\n",
    "<ul>\n",
    "<li>اختصارات مانند U.S.A.</li>\n",
    "<li>اعداد اعشاری مانند 12.40</li>\n",
    "<li>کلمات خط‌چسبیده مانند poster-print</li>\n",
    "<li>نقاط متوالی مانند \"...\"</li>\n",
    "</ul>\n",
    "\n",
    "<strong>تحلیل:</strong><br>\n",
    "- توکنایزر اولیه هر یک از موارد بالا را به توکن‌های جداگانه تقسیم می‌کرد، که مطابق انتظار نبود.<br>\n",
    "- نیاز بود الگو و منطق توکنایزر به گونه‌ای طراحی شود که این موارد به صورت یک توکن نگهداری شوند.<br>\n",
    "\n",
    "<strong>راه حل:</strong><br>\n",
    "- استفاده از الگوهای Regex پیشرفته که اختصارات، اعداد اعشاری، کلمات خط‌چسبیده و نقاط متوالی را شناسایی کنند.<br>\n",
    "- حفظ سایر علائم نگارشی به صورت جداگانه.<br>\n",
    "\n",
    "<strong>پیاده‌سازی کلاس‌محور:</strong><br>\n",
    "- استفاده از کلاس برای توکنایزر باعث می‌شود دایره لغات (Vocabulary) حفظ شود و توکن‌های جدید به صورت پویا اضافه شوند.<br>\n",
    "- هر توکن یک شناسه یکتا دریافت می‌کند و این شناسه بین جملات مختلف پایدار باقی می‌ماند.<br>\n",
    "\n",
    "\n",
    "<strong>ویژگی‌های نهایی توکنایزر:</strong><br>\n",
    "<ul>\n",
    "<li>توانایی تشخیص و حفظ اختصارات مانند U.S.A.</li>\n",
    "<li>توانایی تشخیص اعداد اعشاری مانند 12.40 به عنوان یک توکن</li>\n",
    "<li>حفظ کلمات خط‌چسبیده مانند poster-print</li>\n",
    "<li>تشخیص نقاط متوالی (...) به عنوان یک توکن</li>\n",
    "<li>حفظ و تشخیص سایر علائم نگارشی</li>\n",
    "<li>حفظ دایره لغات و شناسه‌های یکتا برای توکن‌ها</li>\n",
    "<li>قابلیت حساسیت یا عدم حساسیت به حروف بزرگ و کوچک</li>\n",
    "</ul>\n",
    "\n",
    "این روش کلاس‌محور و استفاده از Regex بهبود یافته باعث می‌شود توکنایزر، متونی با ساختارهای مختلف را به صورت دقیق و پایدار تحلیل کند و توکن‌های یکتا و قابل استفاده در پردازش‌های بعدی ایجاد نماید.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">BPE Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ابتدا دیتاست TinyStories-Farsi را از HuggingFace لود کنید.\n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    (لینک دیتاست)\n",
    "</a>\n",
    "<br>\n",
    "سپس از داده آموزش (train) آن جملات فارسی را در یک لیست اضافه کنید.\n",
    "<br>\n",
    "اکنون با استفاده از این مجموعه داده، یک توکنایزر BPE آموزش دهید. استفاده از کتابخانه‌های آماده مانعی ندارد.\n",
    "<br>\n",
    "عملکرد توکنایزر را روی جمله زیر آزمایش کنید:\n",
    "<br>\n",
    "\"روزی یک مرد ثروتمند، پسر بچه کوچکش را بـه ده برد تا بـه او نشان دهد مردمی که در آنجا زندگی می‌کنند، چقدر فقیر هستند.\"\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- مجموعه داده آموزش<br>\n",
    "- تعداد و لیست توکن‌های ایجادشده برای جمله\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7cd882d0-50e3-4f76-a097-a79270d3ce98)')' thrown while requesting GET https://huggingface.co/datasets/taesiri/TinyStories-Farsi/resolve/87493945c3b0f771d4c115be28e6f5bcd1c1d415/InProgress-TinyStoriesV2-GPT4-train-Translated-To-Farsi-w-Claude-2.0.csv\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 124411/124411 [00:12<00:00, 10263.55 examples/s]\n",
      "Generating validation split: 100%|██████████| 27630/27630 [00:01<00:00, 18376.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from datasets import load_dataset\n",
    "\n",
    "# لود دیتاست TinyStories-Farsi\n",
    "dataset = load_dataset(\"taesiri/TinyStories-Farsi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Persian'],\n",
      "        num_rows: 124411\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Persian'],\n",
      "        num_rows: 27630\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = dataset['train']['Persian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# تعریف مدل BPE\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# تعریف Trainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# آموزش توکنایزر روی جملات فارسی\n",
    "tokenizer.train_from_iterator(train_sentences, trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['روزی', 'یک', 'مرد', 'ثروتمند', '،', 'پسر', 'بچه', 'کوچکش', 'را', 'ب', 'ـ', 'ه', 'ده', 'برد', 'تا', 'ب', 'ـ', 'ه', 'او', 'نشان', 'دهد', 'مردمی', 'که', 'در', 'آنجا', 'زندگی', 'می\\u200cکنند', '،', 'چقدر', 'فقیر', 'هستند', '.']\n",
      "IDs: [451, 298, 498, 2054, 157, 472, 506, 2086, 282, 167, 187, 194, 416, 434, 342, 167, 187, 194, 293, 657, 597, 5467, 310, 294, 924, 484, 761, 157, 1216, 3717, 688, 14]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"روزی یک مرد ثروتمند، پسر بچه کوچکش را بـه ده برد تا بـه او نشان دهد مردمی که در آنجا زندگی می‌کنند، چقدر فقیر هستند.\"\n",
    "\n",
    "output = tokenizer.encode(sentence)\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Wordpiece Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "درباره Wordpiece Tokenizer تحقیق کنید.\n",
    "<br>\n",
    "نحوه آموزش این توکنایزر را به طور دقیق شرح دهید و سپس آن را با BPE مقایسه کنید.\n",
    "<br>\n",
    "اکنون یک توکنایزر Wordpiece بر روی مجموعه داده خود آموزش دهید. استفاده از کتابخانه‌های آماده مانعی ندارد.\n",
    "<br>\n",
    "عملکرد توکنایزر را روی جمله زیر آزمایش کنید:\n",
    "<br>\n",
    "\"روزی یک مرد ثروتمند، پسر بچه کوچکش را بـه ده برد تا بـه او نشان دهد مردمی که در آنجا زندگی می‌کنند، چقدر فقیر هستند.\"\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- تعداد و لیست توکن‌های ایجادشده برای جمله\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# تعریف مدل WordPiece\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# تعریف Trainer با توکن‌های ویژه\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train_from_iterator(train_sentences, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['روزی', 'یک', 'مرد', 'ثروتمند', '،', 'پسر', 'بچه', 'کوچکش', 'را', 'ب', '##ـ', '##ه', 'ده', 'برد', 'تا', 'ب', '##ـ', '##ه', 'او', 'نشان', 'دهد', 'مردمی', 'که', 'در', 'آنجا', 'زندگی', 'می\\u200cکنند', '،', 'چقدر', 'فقیر', 'هستند', '.']\n",
      "IDs: [695, 529, 755, 2479, 157, 799, 883, 2514, 520, 167, 332, 285, 1830, 697, 602, 167, 332, 285, 523, 976, 1030, 6190, 543, 534, 1178, 735, 1027, 157, 1594, 4271, 952, 14]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"روزی یک مرد ثروتمند، پسر بچه کوچکش را بـه ده برد تا بـه او نشان دهد مردمی که در آنجا زندگی می‌کنند، چقدر فقیر هستند.\"\n",
    "\n",
    "output = tokenizer.encode(sentence)\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "# آموزش توکنایزر WordPiece و مقایسه با BPE\n",
    "\n",
    "## ۱. مراحل آموزش توکنایزر WordPiece\n",
    "\n",
    "WordPiece یک الگوریتم توکنیزیشن است که برای تبدیل جملات به زیرکلمات استفاده می‌شود و در مدل‌هایی مانند BERT کاربرد دارد. مراحل آموزش به صورت دقیق به شرح زیر است:\n",
    "\n",
    "1. **جمع‌آوری داده‌های متنی**\n",
    "   - ابتدا جملات آموزش را جمع‌آوری می‌کنیم. در مثال ما از دیتاست **TinyStories-Farsi** استفاده شد و ستون `Persian` استخراج شد.\n",
    "   - این مجموعه داده شامل هزاران جمله فارسی است.\n",
    "\n",
    "2. **پیش‌توکنیزیشن (Pre-tokenization)**\n",
    "   - قبل از آموزش، متن به بخش‌های اولیه تقسیم می‌شود. برای فارسی اغلب از فاصله (`Whitespace`) استفاده می‌کنیم.\n",
    "   - مثال:\n",
    "     ```\n",
    "     \"روزی یک مرد\"\n",
    "     ```\n",
    "     بعد از پیش‌توکنیزیشن:\n",
    "     ```\n",
    "     [\"روزی\", \"یک\", \"مرد\"]\n",
    "     ```\n",
    "\n",
    "3. **تعریف مدل WordPiece**\n",
    "   - مدل با استفاده از کلاس `WordPiece` ساخته می‌شود.\n",
    "   - مشخص می‌کنیم که برای توکن‌های ناشناخته از `[UNK]` استفاده شود.\n",
    "\n",
    "4. **تعریف Trainer**\n",
    "   - Trainer مسئول آموزش مدل روی داده‌ها است.\n",
    "   - توکن‌های خاص (special tokens) مانند `[UNK]`, `[CLS]`, `[SEP]`, `[PAD]`, `[MASK]` اضافه می‌شوند تا برای مدل‌های NLP آماده باشند.\n",
    "\n",
    "5. **آموزش مدل**\n",
    "   - با استفاده از متد `train_from_iterator` مدل روی جملات آموزش داده می‌شود.\n",
    "   - الگوریتم WordPiece با بررسی فراوانی زیرکلمات، دایره لغات را می‌سازد و توکن‌ها را بر اساس بیشترین احتمال تقسیم می‌کند.\n",
    "   - **نحوه کار الگوریتم:**  \n",
    "     الگوریتم ابتدا همه کلمات را به کاراکترها تقسیم می‌کند و سپس جفت کاراکترها یا زیرکلمات پرتکرار را ترکیب می‌کند، اما تنها ترکیب‌هایی که بیشترین احتمال و پوشش داده را دارند انتخاب می‌شوند. وقتی جمله جدید وارد می‌شود، کلمات به **طولانی‌ترین زیرکلمات موجود در دایره لغات** تقسیم می‌شوند و `[UNK]` تنها در موارد نادر استفاده می‌شود.\n",
    "\n",
    "6. **تست و ارزیابی**\n",
    "   - جمله‌ای جدید به مدل داده می‌شود و به زیرکلمات تقسیم می‌شود.\n",
    "   - شناسه‌های توکن‌ها (`IDs`) و خود توکن‌ها بررسی می‌شوند.\n",
    "\n",
    "---\n",
    "\n",
    "## ۲. مقایسه WordPiece با BPE\n",
    "\n",
    "| ویژگی | WordPiece | BPE |\n",
    "|--------|-----------|-----|\n",
    "| **مبنای تقسیم** | انتخاب زیرکلمات با بیشترین احتمال. الگوریتم با بررسی فراوانی زیرکلمات، طولانی‌ترین توکن‌ها را انتخاب می‌کند تا پوشش داده حداکثر شود و `[UNK]` کم استفاده شود. | ترکیب زوج‌های کاراکتری پرتکرار |\n",
    "| **کاربرد** | مدل‌های پیش‌آموزش‌دیده مانند BERT | مدل‌های عمومی و GPT-like |\n",
    "| **توکن‌های ناشناخته** | `[UNK]` برای کلمات جدید | `[UNK]` یا توکن ناشناخته مشابه |\n",
    "| **تقسیم کلمات جدید** | ممکن است به زیرکلمات کوچک‌تر تقسیم شود | ترکیب تکرار زوج‌های کاراکتری تا ایجاد توکن |\n",
    "| **حفظ دایره لغات** | کلاس محور امکان اضافه کردن پویا و حفظ شناسه‌ها را دارد | مشابه WordPiece |\n",
    "| **زبان فارسی** | مناسب با پیش‌توکنیزیشن Whitespace و زیرکلمات | مشابه WordPiece، اما ممکن است توکن‌ها بزرگ‌تر باشند |\n",
    "\n",
    "---\n",
    "### ۲️⃣ تفاوت در شناسه‌ها (IDs)\n",
    "\n",
    "- هر توکن در Vocabulary یک شناسه (ID) دارد.\n",
    "- WordPiece معمولاً شناسه‌های متفاوتی برای زیرکلمات با `##` دارد، بنابراین دایره لغاتش کمی بزرگ‌تر و دقیق‌تر است.\n",
    "- BPE شناسه‌ها برای بخش‌های مشابه ساده‌تر هستند.\n",
    "### 🔹 نکات مهم\n",
    "\n",
    "- WordPiece در جملات فارسی زیرکلمات را به صورت هوشمند تقسیم می‌کند، بنابراین مدل‌های BERT فارسی عملکرد بهتری روی داده‌های کوچک و کلمات ناشناخته دارند.\n",
    "- کلاس توکنایزر باعث می‌شود دایره لغات همیشه به‌روزرسانی شود و شناسه توکن‌ها بین جملات پایدار بماند.\n",
    "- هر دو الگوریتم نیاز به پیش‌پردازش متن و حذف نویز دارند تا عملکرد بهینه داشته باشند.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Tokenization Visualization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "با استفاده از ابزار \n",
    "<a href=\"https://tiktokenizer.vercel.app/\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    tiktokenizer\n",
    "</a>\n",
    "توکن‌های تولید شده برای جمله زیر را در هر یک از مدل‌های gpt2 و gpt4 و Meta-Llama-3-8B را مشاهده و تفاوت‌ها را گزارش کنید.\n",
    "<br>\n",
    "\"روزی یک مرد ثروتمند، پسر بچه کوچکش را بـه ده برد تا بـه او نشان دهد مردمی که در آنجا زندگی می‌کنند، چقدر فقیر هستند.\"\n",
    "<br>\n",
    "سپس در مورد توکنایزر استفاده شده در هر یک تحقیق کنید. به نظر شما علت تفاوت نتیجه آن‌ها در چیست؟\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- توکن‌های ایجادشده با هر توکنایزر برای جمله\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "<img src=\"images/all.png\" alt=\"WordPiece vs BPE\" width=\"1000\">\n",
    "\n",
    "### مشاهده توکن‌ها در هر مدل\n",
    "\n",
    "- **Meta-LLaMA-3-8B:**  \n",
    "  - توکن‌ها بیشتر به صورت **کلمات معنی‌دار فارسی** تقسیم شده‌اند.  \n",
    "  - مثال: \"روز\"، \"یک\"، \"مرد\"، \"پسر\"\"، …  \n",
    "  - زیرکلمات (`subwords`) به نحوی تقسیم شده‌اند که معنی کلمه حفظ شود.  \n",
    "\n",
    "- **GPT-4:**  \n",
    "  - توکن‌ها هنوز قابل قبول هستند ولی برخی کلمات فارسی به **زیرکلمات کوچکتر** تقسیم شده‌اند.  \n",
    "  - مثال: بعضی کلمات با پسوند یا ترکیب‌های غیرکامل جدا شده‌اند.  \n",
    "\n",
    "- **GPT-2:**  \n",
    "  - توکن‌ها بیشتر به کاراکترها یا زیرکلمات کوتاه تقسیم می‌شوند.  \n",
    "  - کلمات معنی‌دار فارسی به خوبی حفظ نشده‌اند.  \n",
    "\n",
    "> ترتیب کیفیت توکنیزیشن:  \n",
    "> Meta-LLaMA-3-8B > GPT-4 > GPT-2\n",
    "\n",
    "---\n",
    "\n",
    "### بررسی توکنایزر استفاده شده در هر مدل\n",
    "\n",
    "| مدل | توکنایزر | ویژگی‌ها |\n",
    "|-----|-----------|----------|\n",
    "| GPT-2 | BPE (Byte-Pair Encoding) | مدل قدیمی‌تر، بر اساس ترکیب زوج‌های کاراکتری پرتکرار، به همین دلیل توکنایزینگ فارسی ضعیف‌تر است. |\n",
    "| GPT-4 | BPE پیشرفته یا Unigram LM | توانایی تقسیم کلمات به زیرکلمات با حفظ معنی بهتر؛ عملکرد روی فارسی بهتر از GPT-2 است. |\n",
    "| Meta-LLaMA-3-8B | SentencePiece (Unigram LM) | طراحی برای چندزبانه و توکنایزینگ معنی‌دار؛ کلمات فارسی به شکل کامل یا ترکیب‌های معنادار تقسیم می‌شوند. |\n",
    "\n",
    "---\n",
    "\n",
    "### علت تفاوت در نتایج\n",
    "\n",
    "1. **روش توکنایزر:**  \n",
    "   - GPT-2 از BPE کلاسیک استفاده می‌کند که برای انگلیسی بهینه شده است، بنابراین فارسی به زیرکلمات کوتاه و غیرمعنادار تقسیم می‌شود.  \n",
    "   - GPT-4 از الگوریتم بهبود یافته BPE یا Unigram LM استفاده می‌کند که زیرکلمات طولانی‌تر و معنی‌دار تولید می‌کند.  \n",
    "   - Meta-LLaMA-3-8B با **SentencePiece (Unigram LM)** طراحی شده و به چندزبانه بودن و حفظ کلمات معنی‌دار اهمیت می‌دهد.\n",
    "\n",
    "2. **پوشش زبانی و داده‌های آموزش:**  \n",
    "   - GPT-2 مدل قدیمی‌تر است و داده‌های فارسی در آموزش آن محدود بوده است.  \n",
    "   - GPT-4 داده‌های بیشتری از فارسی داشته و توکنایزر آن بهتر با ویژگی‌های زبان فارسی سازگار است.  \n",
    "   - Meta-LLaMA-3-8B به طور خاص چندزبانه آموزش داده شده و شامل داده‌های فارسی غنی است، بنابراین توکنایزر آن کلمات معنی‌دار را حفظ می‌کند.\n",
    "\n",
    "3. **طراحی دایره لغات (Vocabulary):**  \n",
    "   - دایره لغات بزرگ‌تر و چندزبانه باعث می‌شود زیرکلمات طولانی‌تر حفظ شوند و `[UNK]` کمتر استفاده شود.\n",
    "\n",
    "---\n",
    "\n",
    "### جمع‌بندی\n",
    "\n",
    "- برای جمله فارسی مورد بررسی، **Meta-LLaMA-3-8B بهترین توکنیزیشن را دارد** و کلمات فارسی به شکل کامل یا زیرکلمات معنی‌دار تقسیم شده‌اند.  \n",
    "- **GPT-4** عملکرد قابل قبول دارد ولی هنوز برخی کلمات به زیرکلمات کوتاه تقسیم می‌شوند.  \n",
    "- **GPT-2** توکنیزینگ ضعیفی برای فارسی دارد و اکثر کلمات به بخش‌های کوچک و غیرمعنادار تقسیم شده‌اند.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">⚜️━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⚜️<br>سوال سوم - <span dir=\"ltr\">N-gram Language Modeling</span> (55)<br>⚜️━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⚜️</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "در این سوال، با N-gram Language Modeling و آن را پیاده‌سازی می‌کنید، با استفاده از آن به تولید متن می‌پردازید. سپس با معیار perplexity و نحوه کاربرد آن آشنا می‌شوید. در نهایت الگوریتم‌های smoothing و با کاربردهای آن آشنا می‌شوید. را پیاده‌سازی می‌کنید\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Data cleaning & Tokenization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "مجموعه داده \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "- که در سوال اول نیز از آن استفاده کردید - را لود کنید.\n",
    "<br>\n",
    "ابتدا دادگان فارسی در مجموعه آموزش (train) آن را تمیز کرده و پیش‌پردازش‌های مورد نیاز را بر روی آن انجام دهید. (با بررسی داده‌ها، مشخص‌کنید که این دادگان به چه پیش‌پردازش‌هایی نیاز دارند.)\n",
    "<br>\n",
    "سپس با استفاده از BPE Tokenizer که بر روی این دادگان آموزش‌داده‌اید، دادگان پردازش‌شده را توکنایز کنید.\n",
    "<br>\n",
    "توکن‌های یک جمله را به انتخاب خود، چاپ کنید.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b>\n",
    "<br>\n",
    "- مجموعه داده تمیزشده فارسی\n",
    "<br>\n",
    "- مجموعه داده توکنایز شده فارسی\n",
    "<br>\n",
    "- توکن‌های یک جمله‌ی دلخواه\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " یک‌روز یک پسربچه کوچولو به اسم بن بود. بن دوست داشت دنیای اطرافش را کشف کند. او چیزهای شگفت‌انگیز زیادی دید، مثل وازه‌های زیبایی که در یک مغازه به نمایش گذاشته شده بودند. یک روز هنگامی‌که بن از مغازه رد می‌شد یک وازه بسیار ویژه را دید. وقتی بن آن را دید مبهوت شد! \n",
      "او گفت:«واو، این واقعا یک وازه شگفت‌انگیز است! آیا می‌توانم آن را بخرم؟»\n",
      "فروشنده لبخند زد و گفت: «بله، البته می‌توانی. می‌توانی آن را به خانه ببری و به همه دوستانت نشان دهی که چقدر شگفت‌انگیز است!»\n",
      "پس بن وازه را به خانه برد و از آن بسیار مفتخر بود! او دوستانش را صدا کرد و وازه شگفت‌انگیز را به آنها نشان داد. همه دوستانش فکر می‌کردند که وازه زیباست و نمی‌توانستند باور کنند که بن چقدر خوش‌شانس است.\n",
      "و این‌گونه بود که بن یک وازه شگفت‌انگیز را در مغازه پیدا کرد! \n",
      "\n",
      " یک روزی، یک سمور آبی معتبر به نام الی وجود داشت. او در یک رودخانه با خانواده اش زندگی می‌کرد. آن‌ها همه دوست داشتند با هم بازی کنند و شنا کنند.  \n",
      "یک روز، مادر الی گفت: «الی، زود باش و برای شام ماهی بگیر!» الی سریع شنا کرد تا ماهی بگیرد. او دوستش اردک را دید. اردک گفت: «سلام الی!» الی گفت: «سلام اردک!» من باید عجله کنم و برای خانواده‌ام ماهی بگیرم.»  \n",
      "در حالی که الی مشغول گرفتن ماهی بود، یک سنگ بزرگ و درخشان پیدا کرد. او فکر کرد: «این ماهی نیست، اما خیلی زیباست!» الی سنگ درخشان را با خود به خانه برد تا به خانواده‌اش نشان دهد. همه به سنگ درخشان نگاه کردند و لبخند زدند. سنگ درخشان همه را خوشحال کرد، و آن‌ها ماهی شام را فراموش کردند.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"taesiri/TinyStories-Farsi\")\n",
    "train_sentences = dataset['train']['Persian']\n",
    "val_sentences = dataset['validation']['Persian']\n",
    "\n",
    "print(train_sentences[0],'\\n')\n",
    "print(train_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_sentence:\n",
      "  ترجمه به فارسی:\n",
      "\n",
      "مجبور نیستی از سگ بلند صدا ترسیده باشی، من ازت محافظت میکنم. خرگوش خاکی با دخترک احساس امنیت زیادی می‌کرد. او خیلی مهربان بود و خرگوش خاکی به زودی به او اعتماد پیدا کرد. او به دخترک تکیه داد و دخترک از او محافظت کرد. خرگوش خاکی بهترین دوستش را پیدا کرده بود.\n",
      "Tokens:\n",
      " ['ترجمه', 'به', 'فارسی', ':', 'مجبور', 'نیستی', 'از', 'سگ', 'بلند', 'صدا', 'ترسیده', 'باشی', '،', 'من', 'ازت', 'محافظت', 'میکنم', '.', 'خرگوش', 'خاکی', 'با', 'دخترک', 'احساس', 'امنیت', 'زیادی', 'می\\u200cکرد', '.', 'او', 'خیلی', 'مهربان', 'بود', 'و', 'خرگوش', 'خاکی', 'به', 'زودی', 'به', 'او', 'اعتماد', 'پیدا', 'کرد', '.', 'او', 'به', 'دخترک', 'تکیه', 'داد', 'و', 'دخترک', 'از', 'او', 'محافظت', 'کرد', '.', 'خرگوش', 'خاکی', 'بهترین', 'دوستش', 'را', 'پیدا', 'کرده', 'بود', '.']\n",
      "IDs:\n",
      " [1016, 285, 860, 26, 2103, 3414, 308, 427, 536, 492, 792, 1241, 157, 347, 5231, 2416, 3655, 14, 703, 5695, 280, 1059, 474, 2570, 614, 392, 14, 293, 338, 666, 302, 195, 703, 5695, 285, 916, 285, 293, 1827, 386, 289, 14, 293, 285, 1059, 4062, 377, 195, 1059, 308, 293, 2416, 289, 14, 703, 5695, 670, 628, 282, 386, 576, 302, 14]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def clean_persian_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "train_sentences_cleaned = [clean_persian_text(s) for s in train_sentences]\n",
    "val_sentences_cleaned = [clean_persian_text(s) for s in val_sentences]\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator(train_sentences_cleaned, trainer=trainer)\n",
    "\n",
    "\n",
    "sample_sentence = dataset['validation']['Persian'][0]\n",
    "output = tokenizer.encode(dataset['validation']['Persian'][0])\n",
    "print(\"sample_sentence:\\n\",sample_sentence)\n",
    "print(\"Tokens:\\n\", output.tokens)\n",
    "print(\"IDs:\\n\", output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "# پیش‌پردازش داده‌های فارسی و آموزش BPE Tokenizer\n",
    "\n",
    "---\n",
    "\n",
    "## ۱️⃣ پیش‌پردازش داده‌ها\n",
    "\n",
    "برای آموزش BPE Tokenizer روی مجموعه داده‌های **TinyStories-Farsi**، پیش‌پردازش زیر توصیه می‌شود:\n",
    "\n",
    "1. **حفظ نیم‌فاصله‌ها (`\\u200c`)**  \n",
    "   - مثال: `شگفت‌انگیز`, `می‌خورد`  \n",
    "   - نیم‌فاصله‌ها معنای کلمات را تغییر می‌دهند، بنابراین باید حفظ شوند.\n",
    "\n",
    "2. **یکپارچه‌سازی خطوط (`\\n`)**  \n",
    "   - خطوط چند جمله‌ای باید به یک متن پیوسته تبدیل شوند.  \n",
    "   - جایگزینی `\\n` با فاصله کافی است.\n",
    "\n",
    "3. **حذف فاصله‌های اضافی**  \n",
    "   - استفاده از `\" \".join(text.split())` برای استانداردسازی فاصله‌ها.\n",
    "\n",
    "4. **حفظ علائم نگارشی مهم**  \n",
    "   - علائم فارسی (`،`, `؟`, `«»`) و انگلیسی (`!`, `.`) باید حفظ شوند.\n",
    "\n",
    "5. **حذف نویسه‌های غیر استاندارد (اختیاری)**  \n",
    "   - کاراکترهای کنترل یا نمادهای غیر چاپی را می‌توان حذف کرد.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">پیاده‌سازی N-gram</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "یک کلاس برای ساخت و آموزش N-gram بنویسید.\n",
    "<br>\n",
    "با استفاده از این کلاس و مجموعه داده توکنایز شده که در بخش قبل آماده کردید، \n",
    "<span dir=\"ltr\"> 2-gram, 4-gram, 8-gram</span>\n",
    "بسازید و روی مجموعه داده خود آموزش دهید.\n",
    "<br>\n",
    "با استفاده از مدل‌های آموزش داده شده، متن‌های 100 توکنی تولید کنید و کیفیت متون تولیدشده را با هم مقایسه کنید و تفاوت عملکرد مدل‌ها از نظر پیوستگی و روانی متون را تحلیل کنید.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- مدل‌های N-gram آموزش یافته\n",
    "<br>\n",
    "- متن‌های 100 توکنی تولیدشده با هر N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Initialize an N-gram language model.\n",
    "        Args:\n",
    "            n (int): The 'N' in N-gram (e.g. 2 for bigram, 3 for trigram).\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "        self.vocab = set()\n",
    "\n",
    "    def _pad_sentence(self, tokens):\n",
    "        \"\"\"\n",
    "        Add <s> start tokens and </s> end token for N-gram context.\n",
    "        Example for trigram: ['<s>', '<s>', ...tokens..., '</s>']\n",
    "        \"\"\"\n",
    "        start_tokens = [\"<s>\"] * (self.n - 1)\n",
    "        return start_tokens + tokens + [\"</s>\"]\n",
    "\n",
    "    def train(self, tokenized_sentences):\n",
    "        \"\"\"\n",
    "        Train the N-gram model on a list of tokenized sentences.\n",
    "        \"\"\"\n",
    "        for sentence in tokenized_sentences:\n",
    "            sentence = self._pad_sentence(sentence)\n",
    "            self.vocab.update(sentence)\n",
    "\n",
    "            for i in range(len(sentence) - self.n + 1):\n",
    "                ngram = tuple(sentence[i:i + self.n])\n",
    "                context = ngram[:-1]\n",
    "                target = ngram[-1]\n",
    "                self.ngram_counts[context][target] += 1\n",
    "                self.context_counts[context] += 1\n",
    "\n",
    "    def generate(self, max_tokens=100):\n",
    "        \"\"\"\n",
    "        Generate text up to max_tokens using the trained model.\n",
    "        \"\"\"\n",
    "        context = (\"<s>\",) * (self.n - 1)\n",
    "        generated = list(context)\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            if context not in self.ngram_counts:\n",
    "                break\n",
    "\n",
    "            possible_next = self.ngram_counts[context]\n",
    "            next_word = random.choices(\n",
    "                list(possible_next.keys()),\n",
    "                weights=list(possible_next.values())\n",
    "            )[0]\n",
    "\n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "\n",
    "            generated.append(next_word)\n",
    "            context = tuple(generated[-(self.n - 1):])  # shift context\n",
    "\n",
    "        return \" \".join(generated[self.n - 1:])\n",
    "\n",
    "    def get_probability(self, context, word):\n",
    "        \"\"\"\n",
    "        Calculate conditional probability P(word | context)\n",
    "        \"\"\"\n",
    "        context = tuple(context)\n",
    "        if self.context_counts[context] == 0:\n",
    "            return 0\n",
    "        return self.ngram_counts[context][word] / self.context_counts[context]\n",
    "        \n",
    "    def perplexity(self, tokenized_sentences):\n",
    "        \"\"\"\n",
    "        Compute perplexity over a list of tokenized sentences using log probabilities.\n",
    "        This version avoids underflow.\n",
    "        \"\"\"\n",
    "        log_prob_sum = 0.0\n",
    "        token_count = 0\n",
    "\n",
    "        for sentence in tokenized_sentences:\n",
    "            sentence = self._pad_sentence(sentence)\n",
    "            token_count += len(sentence) - (self.n - 1)\n",
    "\n",
    "            for i in range(self.n - 1, len(sentence)):\n",
    "                context = tuple(sentence[i - self.n + 1:i])\n",
    "                word = sentence[i]\n",
    "                prob = self.get_probability(context, word)\n",
    "\n",
    "                if prob == 0:\n",
    "                    prob = 1e-8  # smoothing fallback\n",
    "\n",
    "                log_prob_sum += math.log(prob)\n",
    "\n",
    "        avg_log_prob = log_prob_sum / token_count\n",
    "        perplexity_value = math.exp(-avg_log_prob)\n",
    "        return perplexity_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all cleaned sentences\n",
    "train_tokenized_sentences = [\n",
    "    tokenizer.encode(sentence).tokens\n",
    "    for sentence in train_sentences_cleaned\n",
    "]\n",
    "\n",
    "val_tokenized_sentences = [\n",
    "    tokenizer.encode(sentence).tokens\n",
    "    for sentence in val_sentences_cleaned\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "یک‌روز یک پسربچه کوچولو به اسم بن بود. بن دوست داشت دنیای اطرافش را کشف کند. او چیزهای شگفت‌انگیز زیادی دید، مثل وازه‌های زیبایی که در یک مغازه به نمایش گذاشته شده بودند. یک روز هنگامی‌که بن از مغازه رد می‌شد یک وازه بسیار ویژه را دید. وقتی بن آن را دید مبهوت شد! او گفت:«واو، این واقعا یک وازه شگفت‌انگیز است! آیا می‌توانم آن را بخرم؟» فروشنده لبخند زد و گفت: «بله، البته می‌توانی. می‌توانی آن را به خانه ببری و به همه دوستانت نشان دهی که چقدر شگفت‌انگیز است!» پس بن وازه را به خانه برد و از آن بسیار مفتخر بود! او دوستانش را صدا کرد و وازه شگفت‌انگیز را به آنها نشان داد. همه دوستانش فکر می‌کردند که وازه زیباست و نمی‌توانستند باور کنند که بن چقدر خوش‌شانس است. و این‌گونه بود که بن یک وازه شگفت‌انگیز را در مغازه پیدا کرد!\n",
      "['یک\\u200cروز', 'یک', 'پسربچه', 'کوچولو', 'به', 'اسم', 'بن', 'بود', '.', 'بن', 'دوست', 'داشت', 'دنیای', 'اطرافش', 'را', 'کشف', 'کند', '.', 'او', 'چیزهای', 'شگفت\\u200cانگیز', 'زیادی', 'دید', '،', 'مثل', 'وازه', '\\u200cهای', 'زیبایی', 'که', 'در', 'یک', 'مغازه', 'به', 'نمایش', 'گذاشته', 'شده', 'بودند', '.', 'یک', 'روز', 'هنگامی\\u200cکه', 'بن', 'از', 'مغازه', 'رد', 'می\\u200cشد', 'یک', 'وازه', 'بسیار', 'ویژه', 'را', 'دید', '.', 'وقتی', 'بن', 'آن', 'را', 'دید', 'مبهوت', 'شد', '!', 'او', 'گفت', ':«', 'واو', '،', 'این', 'واقعا', 'یک', 'وازه', 'شگفت\\u200cانگیز', 'است', '!', 'آیا', 'می\\u200cتوانم', 'آن', 'را', 'بخرم', '؟»', 'فروشنده', 'لبخند', 'زد', 'و', 'گفت', ':', '«', 'بله', '،', 'البته', 'می\\u200cتوانی', '.', 'می\\u200cتوانی', 'آن', 'را', 'به', 'خانه', 'ببری', 'و', 'به', 'همه', 'دوستانت', 'نشان', 'دهی', 'که', 'چقدر', 'شگفت\\u200cانگیز', 'است', '!»', 'پس', 'بن', 'وازه', 'را', 'به', 'خانه', 'برد', 'و', 'از', 'آن', 'بسیار', 'مفتخر', 'بود', '!', 'او', 'دوستانش', 'را', 'صدا', 'کرد', 'و', 'وازه', 'شگفت\\u200cانگیز', 'را', 'به', 'آنها', 'نشان', 'داد', '.', 'همه', 'دوستانش', 'فکر', 'می\\u200cکردند', 'که', 'وازه', 'زیباست', 'و', 'نمی\\u200cتوانستند', 'باور', 'کنند', 'که', 'بن', 'چقدر', 'خوش\\u200cشانس', 'است', '.', 'و', 'این\\u200cگونه', 'بود', 'که', 'بن', 'یک', 'وازه', 'شگفت\\u200cانگیز', 'را', 'در', 'مغازه', 'پیدا', 'کرد', '!']\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences_cleaned[0])\n",
    "print(train_tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "زمانی که شکر ، بنابراین رفتند . او دوست داشتند . او را که می‌دویدند ، یک سگ خیلی ترسیدند . همه به آن چیز بزرگ از مادرت خيلي هيجان زده بود ، اما می‌خواست با لیزا سرش گذاشت . تپ ه‌ست . سگ لبخند زد و بالاتر و به نام لیلی نگاه کرد . در مورد آن را زنده شدند . آنها تا ابد خوشبخت زندگی می‌کرد . کلاه ایمنی‌اش را پیدا کرد و همه چیز شگفت‌انگیزی یافت شود ، امی حسادت می‌کردند و به حرف بزند . به بیرون از مدادهای رنگی برای بازی کند . او به\n"
     ]
    }
   ],
   "source": [
    "### 2-gram\n",
    "ngram_model_2 = NGramLanguageModel(2)\n",
    "ngram_model_2.train(train_tokenized_sentences)\n",
    "generated_text = ngram_model_2.generate(max_tokens=100)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "به فارسی : یک روز ، آن‌ها تپه‌ای بزرگ با سنگ‌ها و گل‌های زیادی زدند . لیلی لبخند زد و گفت : \" واو ، جالبه \" بن گفت . من یه سگ جادویی هستم .\" میا گفت : \" متاسفیم که تو رو ملاقات کردم .\" ‌میا خوشحال شد و برگشت تا با مادرش درباره بستنی عالی که در برف کمپینگ هستند . یک مرد آنها را چک کند ! آنها ظرف‌ها را بشوید . او یک قصر بزرگ زندگی می‌کرد . جین وسایل زیادی داشت . لیلی و تام دوقلوهایی بودند که دوست داشتند در مارپیچ بازی می‌کردند\n"
     ]
    }
   ],
   "source": [
    "### 4-gram\n",
    "ngram_model_4 = NGramLanguageModel(4)\n",
    "ngram_model_4.train(train_tokenized_sentences)\n",
    "generated_text = ngram_model_4.generate(max_tokens=100)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4-gram\n",
    "ngram_model_8 = NGramLanguageModel(8)\n",
    "ngram_model_8.train(train_tokenized_sentences)\n",
    "generated_text = ngram_model_8.generate(max_tokens=100)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated text (in google colab):\n",
    "مامان و بابا توی آشپزخانه بودند . مامان گریه می‌کرد . \" چه اتفاقی افتاد ؟\" بابا پرسید . مامان یک تقویم را بالا گرفت و گفت : \" نگاه کن مامان . پروانه . از سنگ من خوشش اومده . فکر میکنه زیباست . شاید تام اشتباه می کرد . شاید سنگ من زشت نیست . شاید ویژه است .\" مامانش هم لبخند زد . گفت : \" ممنون تام . تو بهترین دکتر هستی !\" تام دوستی به نام سام داشت . سام هم دوست داشت بازی کند . مکس ، لیلی و سام با اسباب بازی‌ها در"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "# مدل N-gram برای تولید متن فارسی\n",
    "\n",
    "---\n",
    "\n",
    "## ۱️⃣ مقدمه\n",
    "\n",
    "مدل‌های **N-gram** یکی از ساده‌ترین روش‌های آماری در مدل‌سازی زبان هستند.  \n",
    "در این روش احتمال وقوع هر توکن بر اساس **توکن‌های قبلی** محاسبه می‌شود:\n",
    "\n",
    "\\[\n",
    "P(w_t | w_{t-1}, w_{t-2}, ..., w_{t-n+1})\n",
    "\\]\n",
    "\n",
    "به عنوان مثال:\n",
    "- در **2-gram** (bigram): هر کلمه به‌کمک کلمه قبل پیش‌بینی می‌شود.\n",
    "- در **4-gram**: هر کلمه بر اساس ۳ کلمه قبلی پیش‌بینی می‌شود.\n",
    "\n",
    "با افزایش `n`، مدل وابستگی بیشتری به تاریخچه دارد و معمولاً متن روان‌تری تولید می‌کند، اما داده و حافظه بیشتری نیاز دارد.\n",
    "\n",
    "---\n",
    "\n",
    "## ۲️⃣ نقش توکن‌های آغاز و پایان جمله\n",
    "\n",
    "برای آموزش بهتر مدل N-gram، لازم است که مرز آغاز و پایان جمله مشخص شود.  \n",
    "بنابراین معمولاً از توکن‌های خاصی مانند `<s>` و `</s>` استفاده می‌شود:\n",
    "\n",
    "- `<s>` نشان‌دهنده‌ی **شروع جمله** است.\n",
    "- `</s>` نشان‌دهنده‌ی **پایان جمله** است.\n",
    "\n",
    "به عنوان مثال در یک مدل **Trigram (3-gram)**، جمله‌ی زیر:\n",
    "\n",
    "```\n",
    "من به مدرسه رفتم\n",
    "```\n",
    "\n",
    "به صورت زیر بازنویسی می‌شود:\n",
    "\n",
    "```\n",
    "<s> <s> من به مدرسه رفتم </s>\n",
    "```\n",
    "\n",
    "و مدل از این توالی برای یادگیری احتمالات استفاده می‌کند.  \n",
    "\n",
    "---\n",
    "\n",
    "## ۳️⃣ فرایند آموزش\n",
    "\n",
    "1. **توکن‌سازی داده‌ها:**  \n",
    "   ابتدا مجموعه داده با استفاده از یک **توکنایزر (مثلاً BPE)** به توکن‌ها شکسته می‌شود.\n",
    "\n",
    "2. **ساخت جداول n-gram:**  \n",
    "   برای هر جمله، n-gram‌ها استخراج می‌شوند.  \n",
    "   به عنوان مثال در bigram:\n",
    "\n",
    "   ```python\n",
    "   [('من', 'به'), ('به', 'مدرسه'), ('مدرسه', 'رفتم')]\n",
    "   ```\n",
    "\n",
    "3. **محاسبه احتمال شرطی:**  \n",
    "   احتمال هر توکن جدید بر اساس تاریخچه‌اش محاسبه می‌شود:\n",
    "\n",
    "   \\[\n",
    "   P(w_i | w_{i-1}) = \\frac{Count(w_{i-1}, w_i)}{Count(w_{i-1})}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## ۴️⃣ تولید متن با مدل N-gram\n",
    "\n",
    "برای تولید متن جدید، مراحل زیر انجام می‌شود:\n",
    "\n",
    "1. از توکن `<s>` شروع می‌کنیم.\n",
    "2. بر اساس احتمال شرطی، توکن بعدی را انتخاب می‌کنیم.\n",
    "3. این فرآیند تا رسیدن به `</s>` یا طول مشخصی از متن ادامه می‌یابد.\n",
    "\n",
    "با افزایش `n` (مثلاً از 2-gram به 8-gram)،  \n",
    "مدل زمینه‌ی بیشتری در نظر می‌گیرد و متون **پیوسته‌تر و طبیعی‌تر** تولید می‌کند،  \n",
    "اما به داده‌ی بیشتری نیاز دارد و احتمال overfitting بالاتر می‌رود.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ نکات کلیدی\n",
    "\n",
    "- افزودن `<s>` و `</s>` باعث بهبود درک مدل از مرز جمله‌ها می‌شود.  \n",
    "- مدل‌های N-gram پایه‌ای هستند، اما برای زبان فارسی می‌توانند ساختار جمله را تا حد خوبی یاد بگیرند.  \n",
    "- ترکیب آن‌ها با تکنیک‌هایی مانند **smoothing** و **backoff** موجب بهبود عملکرد می‌شود.\n",
    "\n",
    "---\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">معیار Perplexity</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ابتدا از مجموعه داده \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "دادگان validation فارسی را جدا کنید.\n",
    "<br>\n",
    "سپس معیار Perplexity را برای هر کدام از N-gram های خود، روی این مجموعه حساب کنید.\n",
    "<br>\n",
    "تحلیل خود را از این نتیجه بگویید.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- مجموعه دادگان فارسی validation\n",
    "<br>\n",
    "- نتیجه معیار Perplexity در هر N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_perplexity = ngram_model_2.perplexity(val_tokenized_sentences)\n",
    "print(\"2-gram perplexity:\", val_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color:#0f700c'>[Run in colab] output: </div> 2-gram perplexity: 91.37720396423376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_perplexity = ngram_model_4.perplexity(val_tokenized_sentences)\n",
    "print(\"4-gram perplexity:\", val_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color:#0f700c'>[Run in colab] output: </div> 4-gram perplexity: 3447.3959462310636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_perplexity = ngram_model_8.perplexity(val_tokenized_sentences)\n",
    "print(\"8-gram perplexity:\", val_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color:#0f700c'>[Run in colab] output: </div> 8-gram perplexity: 8325176.238196614"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "✍️ <b>پاسخ تشریحی زیربخش سوم:</b><br>\n",
    "{{پاسخ_خود_را_اینجا_بنویسید}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "در این قسمت چهار جمله در اختیار شما قرار گرفته است. استدلال کنید کدام جمله‌ها محتمل‌ترند که توسط یک <span dir=\"ltr\">4-gram</span> که روی داده‌های مشابه آموزش دیده‌است، تولید شده باشند.\n",
    "<br>\n",
    "جمله‌ی اول = آنها دوست داشتند در ماسه بازی کنند و جزر و مد آب را تماشا کنند\n",
    "<br>\n",
    "جمله‌ی دوم = جیل و تام به همراه مامان و بابا به ساحل رفتند\n",
    "<br>\n",
    "جمله‌ی سوم = تام بطری‌ نوشابه‌ را تا حد ممکن بالا انداخت و به سمت او فریاد زد\n",
    "<br>\n",
    "جمله‌ی چهارم = باری خیلی دوست داشت بیرون از منزل نقاشی کند و با پدربزرگ منظره تماشا کند\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "✍️ <b>پاسخ تشریحی زیربخش چهارم:</b><br>\n",
    "{{پاسخ_خود_را_اینجا_بنویسید}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">روش‌های هموارسازی</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "هر کدام از الگوریتم‌های هموارسازی (Smoothing) که در درس خوانده‌اید (Laplace, Interpolation, Backoff) را پیاده‌سازی کنید.\n",
    "<br>\n",
    "یک مدل\n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "بسازید و با مجموعه داده توکنایزشده فارسی که در بخش اول این سوال آماده کردید، آموزش دهید.\n",
    "(می‌توانید از مدل \n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "آموزش‌یافته قبلی خود استفاده کنید.)\n",
    "<br>\n",
    "در الگوریتم Interpolation مقادیر λ را 0.4, 0.3, 0.2, 0.1 در نظر بگیرید. به این صورت:\n",
    "P​(w∣h3​,h2​,h1​)=0.4P​(w∣h3​,h2​,h1​)+0.3P​(w∣h2​,h1​)+0.2P​(w∣h1​)+0.1P​(w)\n",
    "<br>\n",
    "در الگوریتم Backoff مقدار λ را 0.4 در نظر بگیرید.\n",
    "<br>\n",
    "سپس با استفاده از هر یک از این روش‌ها، متن‌هایی به طول 100 توکن تولید کنید.\n",
    "<br>\n",
    "متن‌های تولیدشده را با یکدیگر مقایسه کنید و تفاوت آن‌ها را از نظر روانی و تنوع کلمات بررسی کنید.\n",
    "<br>\n",
    "در نهایت، با استفاده از احتمالات محاسبه‌شده در هر یک از این روش‌ها، معیار Perplexity را روی مجموعه دادگان فارسی validation - که در بخش سوم این سوال آماده کردید - حساب کنید.\n",
    "<br>\n",
    "مقادیر Perplexity برای هر کدام از روش‌های هموارسازی و مدل غیرهموار (Unsmoothed) را با یکدیگر مقایسه کنید و تحلیل خود را از این نتایج بگویید.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- متن‌های تولیدشده با هر یک از روش‌های هموارسازی ذکر شده\n",
    "<br>\n",
    "- معیار Perplexity برای هر یک از روش‌های هموارسازی ذکر شده\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "✍️ <b>پاسخ تشریحی زیربخش پنجم:</b><br>\n",
    "{{پاسخ_خود_را_اینجا_بنویسید}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">پیاده‌سازی Temperature</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "در مورد تاثیر Temperature در مدل‌های زبانی توضیح‌دهید.\n",
    "<br>\n",
    "به هنگام Sampling از N-gram، برای آن Temperature تعریف کنید و پیاده‌سازی‌های لازم را انجام دهید.\n",
    "<br>\n",
    "با قرار دادن کمترین Temperature و با بیشترین Temperature، سه‌بار خروجی‌هایی به طول ۲۰ توکن تولید کنید. (برای هر حالت سه‌بار) و سپس تاثیر Temperature در خروجی‌ها را تحلیل کنید.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "🎯 <b>خروجی مورد انتظار:</b><br>\n",
    "- متن‌های تولید‌شده با Temperature بالا و پایین (برای هریک ۳ عدد متن تولید شده)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "✍️ <b>پاسخ تشریحی زیربخش ششم:</b><br>\n",
    "{{پاسخ_خود_را_اینجا_بنویسید}}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**نکات مهم و قوانین تحویل**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px; text-align: right;\">\n",
    "    <p style=\"text-align: right;\" dir=\"rtl\"><strong dir=\"rtl\">مهلت تحویل :</strong> 10 آبان</p>\n",
    "</div>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">فایل ارسالی شما باید با فرمت زیر نامگذاری شود: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">نحوه انجام تمرین:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>سلول‌های کد با برچسب <code>WRITE YOUR CODE HERE</code> را تکمیل کنید.</li>\n",
    "  <li>برای پاسخ‌های متنی، متن <code>{{پاسخ_خود_را_اینجا_بنویسید}}</code> را با پاسخ خود جایگزین کنید.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">صداقت علمی:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>ما نوت‌بوک‌های تعداد مشخصی از دانشجویان که به صورت تصادفی انتخاب می‌شوند، بررسی خواهیم کرد. این بررسی‌ها اطمینان حاصل می‌کنند که کدی که نوشتید واقعاً پاسخ‌های موجود در نوت‌بوک شما را تولید می‌کند. اگر پاسخ‌های صحیح را در نوت‌بوک خود بدون کدی که واقعاً آن پاسخ‌ها را تولید کند تحویل دهید، این یک مورد جدی از عدم صداقت علمی محسوب می‌شود.</li> <li>ما همچنین بررسی‌های خودکاری را برای تشخیص سرقت علمی در نوت‌بوک‌های کولب انجام خواهیم داد. کپی کردن کد از دیگران نیز یک مورد جدی از عدم صداقت علمی محسوب می‌شود.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">توضیحات تکمیلی:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "خوانایی و دقت بررسی‌ها در گزارش نهایی از اهمیت ویژه‌ای برخوردار است. به تمرین‌هایی که به صورت کاغذی تحویل داده شوند یا به صورت عکس در سایت بارگذاری شوند، ترتیب اثری داده نخواهد شد.</li>\n",
    "<li>\n",
    " همه‌ی کدهای پیوست گزارش بایستی قابلیت اجرای مجدد داشته باشند. در صورتی که برای اجرا مجدد آن‌ها نیاز به تنظیمات خاصی می‌باشد، بایستی تنظیمات مورد نیاز را نیز در گزارش خود ذکر کنید.  دقت کنید که  تمامی کدها باید توسط شما اجرا شده باشند و نتایج اجرا در فایل کدهای ارسالی مشخص باشد. به کدهایی که نتایج اجرای آن‌ها در فایل ارسالی مشخص نباشد نمره‌ای تعلق نمی‌گیرد.\n",
    "</li>\n",
    "<li>توجه کنید این تمرین باید به صورت تک‌نفره انجام شود و پاسخ‌های ارائه شده باید نتیجه فعالیت فرد نویسنده باشد (همفکری و به اتفاق هم نوشتن تمرین نیز ممنوع است). در صورت مشاهده\n",
    " تشابه به همه افراد مشارکت‌کننده، نمره تمرین صفر و به استاد گزارش می‌گردد.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "لطفاً تمامی پاسخ‌های متنی خود را با <b>فونت وزیر (Vazir)</b> و به‌صورت <b>راست‌چین</b> بنویسید.  \n",
    "از استفاده از فونت‌های پیش‌فرض خودداری کنید تا ظاهر نوت‌بوک شما یک‌دست و خوانا باشد.  \n",
    "در بخش‌های تشریحی، سعی کنید پاسخ‌ها را کامل، منسجم و با رعایت نگارش فارسی بنویسید.  \n",
    "همچنین، به چینش تمیز سلول‌ها و اجرای درست کدها توجه کنید تا تمرین شما با فرمت خواسته‌شده و استاندارد ارائه شود.\n",
    "</li>\n",
    " <li>برای مطالعه بیشتر درباره‌ی فرمت Markdown می‌توانید از <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">این لینک</a> مطالعه کنید.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
